<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Chloe">


    <meta name="subtitle" content="有些人能收到雨，而其他则只是被淋湿。">




<title>Task4-Modeling | Chloe&#39;s Blog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Chloe&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/category">Categories[分类]</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Chloe&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/category">Categories[分类]</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Task4-Modeling</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Chloe</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">April 1, 2020&nbsp;&nbsp;20:36:41</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="Datawhale-零基础入门数据挖掘-Task4-建模调参"><a href="#Datawhale-零基础入门数据挖掘-Task4-建模调参" class="headerlink" title="Datawhale 零基础入门数据挖掘-Task4 建模调参"></a>Datawhale 零基础入门数据挖掘-Task4 建模调参</h1><h2 id="四、建模与调参"><a href="#四、建模与调参" class="headerlink" title="四、建模与调参"></a>四、建模与调参</h2><p>Tip:此部分为零基础入门数据挖掘的 Task4 建模调参 部分，带你来了解各种模型以及模型的评价和调参策略，欢迎大家后续多多交流。</p>
<p><strong>赛题：零基础入门数据挖掘 - 二手车交易价格预测</strong></p>
<p>地址：<a href="https://tianchi.aliyun.com/competition/entrance/231784/introduction?spm=5176.12281957.1004.1.38b02448ausjSX" target="_blank" rel="noopener">https://tianchi.aliyun.com/competition/entrance/231784/introduction?spm=5176.12281957.1004.1.38b02448ausjSX</a> </p>
<h2 id="5-1-学习目标"><a href="#5-1-学习目标" class="headerlink" title="5.1 学习目标"></a>5.1 学习目标</h2><ul>
<li>了解常用的机器学习模型，并掌握机器学习模型的建模与调参流程</li>
<li>完成相应学习打卡任务</li>
</ul>
<h2 id="5-2-内容介绍"><a href="#5-2-内容介绍" class="headerlink" title="5.2 内容介绍"></a>5.2 内容介绍</h2><ol>
<li>线性回归模型：<ul>
<li>线性回归对于特征的要求；</li>
<li>处理长尾分布；</li>
<li>理解线性回归模型；</li>
</ul>
</li>
<li>模型性能验证：<ul>
<li>评价函数与目标函数；</li>
<li>交叉验证方法；</li>
<li>留一验证方法；</li>
<li>针对时间序列问题的验证；</li>
<li>绘制学习率曲线；</li>
<li>绘制验证曲线；</li>
</ul>
</li>
<li>嵌入式特征选择：<ul>
<li>Lasso回归；</li>
<li>Ridge回归；</li>
<li>决策树；</li>
</ul>
</li>
<li>模型对比：<ul>
<li>常用线性模型；</li>
<li>常用非线性模型；</li>
</ul>
</li>
<li>模型调参：<ul>
<li>贪心调参方法；</li>
<li>网格调参方法；</li>
<li>贝叶斯调参方法；</li>
</ul>
</li>
</ol>
<h2 id="5-3-相关原理介绍与推荐"><a href="#5-3-相关原理介绍与推荐" class="headerlink" title="5.3 相关原理介绍与推荐"></a>5.3 相关原理介绍与推荐</h2><p>由于相关算法原理篇幅较长，本文推荐了一些博客与教材供初学者们进行学习。</p>
<h3 id="5-3-1-线性回归模型"><a href="#5-3-1-线性回归模型" class="headerlink" title="5.3.1 线性回归模型"></a>5.3.1 线性回归模型</h3><p><a href="https://zhuanlan.zhihu.com/p/49480391" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49480391</a></p>
<h3 id="5-3-2-决策树模型"><a href="#5-3-2-决策树模型" class="headerlink" title="5.3.2 决策树模型"></a>5.3.2 决策树模型</h3><p><a href="https://zhuanlan.zhihu.com/p/65304798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/65304798</a></p>
<h3 id="5-3-3-GBDT模型"><a href="#5-3-3-GBDT模型" class="headerlink" title="5.3.3 GBDT模型"></a>5.3.3 GBDT模型</h3><p><a href="https://zhuanlan.zhihu.com/p/45145899" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/45145899</a></p>
<h3 id="5-3-4-XGBoost模型"><a href="#5-3-4-XGBoost模型" class="headerlink" title="5.3.4 XGBoost模型"></a>5.3.4 XGBoost模型</h3><p><a href="https://zhuanlan.zhihu.com/p/86816771" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/86816771</a></p>
<h3 id="5-3-5-LightGBM模型"><a href="#5-3-5-LightGBM模型" class="headerlink" title="5.3.5 LightGBM模型"></a>5.3.5 LightGBM模型</h3><p><a href="https://zhuanlan.zhihu.com/p/89360721" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/89360721</a></p>
<h3 id="5-3-6-推荐教材："><a href="#5-3-6-推荐教材：" class="headerlink" title="5.3.6 推荐教材："></a>5.3.6 推荐教材：</h3><ul>
<li>《机器学习》 <a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">https://book.douban.com/subject/26708119/</a></li>
<li>《统计学习方法》 <a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">https://book.douban.com/subject/10590856/</a></li>
<li>《Python大战机器学习》 <a href="https://book.douban.com/subject/26987890/" target="_blank" rel="noopener">https://book.douban.com/subject/26987890/</a></li>
<li>《面向机器学习的特征工程》 <a href="https://book.douban.com/subject/26826639/" target="_blank" rel="noopener">https://book.douban.com/subject/26826639/</a></li>
<li>《数据科学家访谈录》 <a href="https://book.douban.com/subject/30129410/" target="_blank" rel="noopener">https://book.douban.com/subject/30129410/</a></li>
</ul>
<h2 id="5-4-代码示例"><a href="#5-4-代码示例" class="headerlink" title="5.4 代码示例"></a>5.4 代码示例</h2><h3 id="5-4-1-读取数据"><a href="#5-4-1-读取数据" class="headerlink" title="5.4.1 读取数据"></a>5.4.1 读取数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure>

<p>reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_mem_usage</span><span class="params">(df)</span>:</span></span><br><span class="line">    <span class="string">""" iterate through all the columns of a dataframe and modify the data type</span></span><br><span class="line"><span class="string">        to reduce memory usage.        </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    start_mem = df.memory_usage().sum() </span><br><span class="line">    print(<span class="string">'Memory usage of dataframe is &#123;:.2f&#125; MB'</span>.format(start_mem))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtype</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> col_type != object:</span><br><span class="line">            c_min = df[col].min()</span><br><span class="line">            c_max = df[col].max()</span><br><span class="line">            <span class="keyword">if</span> str(col_type)[:<span class="number">3</span>] == <span class="string">'int'</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int8).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).min <span class="keyword">and</span> c_max &lt; np.iinfo(np.int64).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)  </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).min <span class="keyword">and</span> c_max &lt; np.finfo(np.float16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).min <span class="keyword">and</span> c_max &lt; np.finfo(np.float32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            df[col] = df[col].astype(<span class="string">'category'</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().sum() </span><br><span class="line">    print(<span class="string">'Memory usage after optimization is: &#123;:.2f&#125; MB'</span>.format(end_mem))</span><br><span class="line">    print(<span class="string">'Decreased by &#123;:.1f&#125;%'</span>.format(<span class="number">100</span> * (start_mem - end_mem) / start_mem))</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature = reduce_mem_usage(pd.read_csv(<span class="string">'data_for_tree.csv'</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Memory usage of dataframe is 60507328.00 MB
Memory usage after optimization is: 15724107.00 MB
Decreased by 74.0%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">continuous_feature_names = [x <span class="keyword">for</span> x <span class="keyword">in</span> sample_feature.columns <span class="keyword">if</span> x <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'price'</span>,<span class="string">'brand'</span>,<span class="string">'model'</span>,<span class="string">'brand'</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="5-4-2-线性回归-amp-五折交叉验证-amp-模拟真实业务情况"><a href="#5-4-2-线性回归-amp-五折交叉验证-amp-模拟真实业务情况" class="headerlink" title="5.4.2 线性回归 &amp; 五折交叉验证 &amp; 模拟真实业务情况"></a>5.4.2 线性回归 &amp; 五折交叉验证 &amp; 模拟真实业务情况</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sample_feature = sample_feature.dropna().replace(<span class="string">'-'</span>, <span class="number">0</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">sample_feature[<span class="string">'notRepairedDamage'</span>] = sample_feature[<span class="string">'notRepairedDamage'</span>].astype(np.float32)</span><br><span class="line">train = sample_feature[continuous_feature_names + [<span class="string">'price'</span>]]</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y = train[<span class="string">'price'</span>]</span><br></pre></td></tr></table></figure>

<h4 id="5-4-2-1-简单建模"><a href="#5-4-2-1-简单建模" class="headerlink" title="5.4.2 - 1 简单建模"></a>5.4.2 - 1 简单建模</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = LinearRegression(normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = model.fit(train_X, train_y)</span><br></pre></td></tr></table></figure>

<p>查看训练的线性回归模型的截距（intercept）与权重(coef)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'intercept:'</span>+ str(model.intercept_)</span><br><span class="line"></span><br><span class="line">sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>[(&apos;v_6&apos;, 3342612.384537345),
 (&apos;v_8&apos;, 684205.534533214),
 (&apos;v_9&apos;, 178967.94192530424),
 (&apos;v_7&apos;, 35223.07319016895),
 (&apos;v_5&apos;, 21917.550249749802),
 (&apos;v_3&apos;, 12782.03250792227),
 (&apos;v_12&apos;, 11654.925634146672),
 (&apos;v_13&apos;, 9884.194615297649),
 (&apos;v_11&apos;, 5519.182176035517),
 (&apos;v_10&apos;, 3765.6101415594258),
 (&apos;gearbox&apos;, 900.3205339198406),
 (&apos;fuelType&apos;, 353.5206495542567),
 (&apos;bodyType&apos;, 186.51797317460046),
 (&apos;city&apos;, 45.17354204168846),
 (&apos;power&apos;, 31.163045441455335),
 (&apos;brand_price_median&apos;, 0.535967111869784),
 (&apos;brand_price_std&apos;, 0.4346788365040235),
 (&apos;brand_amount&apos;, 0.15308295553300566),
 (&apos;brand_price_max&apos;, 0.003891831020467389),
 (&apos;seller&apos;, -1.2684613466262817e-06),
 (&apos;offerType&apos;, -4.759058356285095e-06),
 (&apos;brand_price_sum&apos;, -2.2430642281682917e-05),
 (&apos;name&apos;, -0.00042591632723759166),
 (&apos;used_time&apos;, -0.012574429533889028),
 (&apos;brand_price_average&apos;, -0.414105722833381),
 (&apos;brand_price_min&apos;, -2.3163823428971835),
 (&apos;train&apos;, -5.392535065078232),
 (&apos;power_bin&apos;, -59.24591853031839),
 (&apos;v_14&apos;, -233.1604256172217),
 (&apos;kilometer&apos;, -372.96600915402496),
 (&apos;notRepairedDamage&apos;, -449.29703564695365),
 (&apos;v_0&apos;, -1490.6790578168238),
 (&apos;v_4&apos;, -14219.648899108111),
 (&apos;v_2&apos;, -16528.55239086934),
 (&apos;v_1&apos;, -42869.43976200439)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subsample_index = np.random.randint(low=<span class="number">0</span>, high=len(train_y), size=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>

<p>绘制特征v_9的值与标签的散点图，图片发现模型的预测结果（蓝色点）与真实标签（黑色点）的分布差异较大，且部分预测值出现了小于0的情况，说明我们的模型存在一些问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(train_X[<span class="string">'v_9'</span>][subsample_index], train_y[subsample_index], color=<span class="string">'black'</span>)</span><br><span class="line">plt.scatter(train_X[<span class="string">'v_9'</span>][subsample_index], model.predict(train_X.loc[subsample_index]), color=<span class="string">'blue'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'v_9'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'price'</span>)</span><br><span class="line">plt.legend([<span class="string">'True Price'</span>,<span class="string">'Predicted Price'</span>],loc=<span class="string">'upper right'</span>)</span><br><span class="line">print(<span class="string">'The predicted price is obvious different from true price'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>The predicted price is obvious different from true price</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321231804889.png" alt="output_22_1"></p>
<p>通过作图我们发现数据的标签（price）呈现长尾分布，不利于我们的建模预测。原因是很多模型都假设数据误差项符合正态分布，而长尾分布的数据违背了这一假设。参考博客：<a href="https://blog.csdn.net/Noob_daniel/article/details/76087829" target="_blank" rel="noopener">https://blog.csdn.net/Noob_daniel/article/details/76087829</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">print(<span class="string">'It is clear to see the price shows a typical exponential distribution'</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.distplot(train_y)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">sns.distplot(train_y[train_y &lt; np.quantile(train_y, <span class="number">0.9</span>)])</span><br></pre></td></tr></table></figure>

<pre><code>It is clear to see the price shows a typical exponential distribution





&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b33efb2f98&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321231820197.png" alt="output_24_2"></p>
<p>在这里我们对标签进行了 $log(x+1)$ 变换，使标签贴近于正态分布</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_y_ln = np.log(train_y + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">print(<span class="string">'The transformed price seems like normal distribution'</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.distplot(train_y_ln)</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">sns.distplot(train_y_ln[train_y_ln &lt; np.quantile(train_y_ln, <span class="number">0.9</span>)])</span><br></pre></td></tr></table></figure>

<pre><code>The transformed price seems like normal distribution





&lt;matplotlib.axes._subplots.AxesSubplot at 0x1b33f077160&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321231840673.png" alt="output_27_2"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = model.fit(train_X, train_y_ln)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'intercept:'</span>+ str(model.intercept_))</span><br><span class="line">sorted(dict(zip(continuous_feature_names, model.coef_)).items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>intercept:23.515920686637713





[(&apos;v_9&apos;, 6.043993029165403),
 (&apos;v_12&apos;, 2.0357439855551394),
 (&apos;v_11&apos;, 1.3607608712255672),
 (&apos;v_1&apos;, 1.3079816298861897),
 (&apos;v_13&apos;, 1.0788833838535354),
 (&apos;v_3&apos;, 0.9895814429387444),
 (&apos;gearbox&apos;, 0.009170812023421397),
 (&apos;fuelType&apos;, 0.006447089787635784),
 (&apos;bodyType&apos;, 0.004815242907679581),
 (&apos;power_bin&apos;, 0.003151801949447194),
 (&apos;power&apos;, 0.0012550361843629999),
 (&apos;train&apos;, 0.0001429273782925814),
 (&apos;brand_price_min&apos;, 2.0721302299502698e-05),
 (&apos;brand_price_average&apos;, 5.308179717783439e-06),
 (&apos;brand_amount&apos;, 2.8308531339942507e-06),
 (&apos;brand_price_max&apos;, 6.764442596115763e-07),
 (&apos;offerType&apos;, 1.6765966392995324e-10),
 (&apos;seller&apos;, 9.308109838457312e-12),
 (&apos;brand_price_sum&apos;, -1.3473184925468486e-10),
 (&apos;name&apos;, -7.11403461065247e-08),
 (&apos;brand_price_median&apos;, -1.7608143661053008e-06),
 (&apos;brand_price_std&apos;, -2.7899058266986454e-06),
 (&apos;used_time&apos;, -5.6142735899344175e-06),
 (&apos;city&apos;, -0.0024992974087053223),
 (&apos;v_14&apos;, -0.012754139659375262),
 (&apos;kilometer&apos;, -0.013999175312751872),
 (&apos;v_0&apos;, -0.04553774829634237),
 (&apos;notRepairedDamage&apos;, -0.273686961116076),
 (&apos;v_7&apos;, -0.7455902679730504),
 (&apos;v_4&apos;, -0.9281349233755761),
 (&apos;v_2&apos;, -1.2781892166433606),
 (&apos;v_5&apos;, -1.5458846136756323),
 (&apos;v_10&apos;, -1.8059217242413748),
 (&apos;v_8&apos;, -42.611729973490604),
 (&apos;v_6&apos;, -241.30992120503035)]</code></pre><p>再次进行可视化，发现预测结果与真实值较为接近，且未出现异常状况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(train_X[<span class="string">'v_9'</span>][subsample_index], train_y[subsample_index], color=<span class="string">'black'</span>)</span><br><span class="line">plt.scatter(train_X[<span class="string">'v_9'</span>][subsample_index], np.exp(model.predict(train_X.loc[subsample_index])), color=<span class="string">'blue'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'v_9'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'price'</span>)</span><br><span class="line">plt.legend([<span class="string">'True Price'</span>,<span class="string">'Predicted Price'</span>],loc=<span class="string">'upper right'</span>)</span><br><span class="line">print(<span class="string">'The predicted price seems normal after np.log transforming'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>The predicted price seems normal after np.log transforming</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321231902283.png" alt="output_30_1"></p>
<h4 id="5-4-2-2-五折交叉验证"><a href="#5-4-2-2-五折交叉验证" class="headerlink" title="5.4.2 - 2 五折交叉验证"></a>5.4.2 - 2 五折交叉验证</h4><blockquote>
<p>在使用训练集对参数进行训练的时候，经常会发现人们通常会将一整个训练集分为三个部分（比如mnist手写训练集）。一般分为：训练集（train_set），评估集（valid_set），测试集（test_set）这三个部分。这其实是为了保证训练效果而特意设置的。其中测试集很好理解，其实就是完全不参与训练的数据，仅仅用来观测测试效果的数据。而训练集和评估集则牵涉到下面的知识了。</p>
</blockquote>
<blockquote>
<p>因为在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证（Cross Validation）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error,  make_scorer</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log_transfer</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapper</span><span class="params">(y, yhat)</span>:</span></span><br><span class="line">        result = func(np.log(y), np.nan_to_num(np.log(yhat)))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores = cross_val_score(model, X=train_X, y=train_y, verbose=<span class="number">1</span>, cv = <span class="number">5</span>, scoring=make_scorer(log_transfer(mean_absolute_error)))</span><br></pre></td></tr></table></figure>

<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished</code></pre><p>使用线性回归模型，对未处理标签的特征数据进行五折交叉验证（Error 1.36）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'AVG:'</span>, np.mean(scores))</span><br></pre></td></tr></table></figure>

<pre><code>AVG: 1.3641908155886227</code></pre><p>使用线性回归模型，对处理过标签的特征数据进行五折交叉验证（Error 0.19）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">1</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error))</span><br></pre></td></tr></table></figure>

<pre><code>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.1s finished</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'AVG:'</span>, np.mean(scores))</span><br></pre></td></tr></table></figure>

<pre><code>AVG: 0.19382863663604424</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scores = pd.DataFrame(scores.reshape(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">scores.columns = [<span class="string">'cv'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">scores.index = [<span class="string">'MAE'</span>]</span><br><span class="line">scores</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cv1</th>
      <th>cv2</th>
      <th>cv3</th>
      <th>cv4</th>
      <th>cv5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MAE</th>
      <td>0.191642</td>
      <td>0.194986</td>
      <td>0.192737</td>
      <td>0.195329</td>
      <td>0.19445</td>
    </tr>
  </tbody>
</table>
</div>



<h4 id="5-4-2-3-模拟真实业务情况"><a href="#5-4-2-3-模拟真实业务情况" class="headerlink" title="5.4.2 - 3 模拟真实业务情况"></a>5.4.2 - 3 模拟真实业务情况</h4><p>但在事实上，由于我们并不具有预知未来的能力，五折交叉验证在某些与时间相关的数据集上反而反映了不真实的情况。通过2018年的二手车价格预测2017年的二手车价格，这显然是不合理的，因此我们还可以采用时间顺序对数据集进行分隔。在本例中，我们选用靠前时间的4/5样本当作训练集，靠后时间的1/5当作验证集，最终结果与五折交叉验证差距不大</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_feature = sample_feature.reset_index(drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">split_point = len(sample_feature) // <span class="number">5</span> * <span class="number">4</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train = sample_feature.loc[:split_point].dropna()</span><br><span class="line">val = sample_feature.loc[split_point:].dropna()</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y_ln = np.log(train[<span class="string">'price'</span>] + <span class="number">1</span>)</span><br><span class="line">val_X = val[continuous_feature_names]</span><br><span class="line">val_y_ln = np.log(val[<span class="string">'price'</span>] + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = model.fit(train_X, train_y_ln)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean_absolute_error(val_y_ln, model.predict(val_X))</span><br></pre></td></tr></table></figure>




<pre><code>0.19443858353490887</code></pre><h4 id="5-4-2-4-绘制学习率曲线与验证曲线"><a href="#5-4-2-4-绘制学习率曲线与验证曲线" class="headerlink" title="5.4.2 - 4 绘制学习率曲线与验证曲线"></a>5.4.2 - 4 绘制学习率曲线与验证曲线</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve, validation_curve</span><br></pre></td></tr></table></figure>


<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">? learning_curve</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, ylim=None, cv=None,n_jobs=<span class="number">1</span>, train_size=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span> )</span>)</span>:</span>  </span><br><span class="line">    plt.figure()  </span><br><span class="line">    plt.title(title)  </span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  </span><br><span class="line">        plt.ylim(*ylim)  </span><br><span class="line">    plt.xlabel(<span class="string">'Training example'</span>)  </span><br><span class="line">    plt.ylabel(<span class="string">'score'</span>)  </span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_size, scoring = make_scorer(mean_absolute_error))  </span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)  </span><br><span class="line">    plt.grid()<span class="comment">#区域  </span></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,  </span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,  </span><br><span class="line">                     color=<span class="string">"r"</span>)  </span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,  </span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>,  </span><br><span class="line">                     color=<span class="string">"g"</span>)  </span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">'r'</span>,  </span><br><span class="line">             label=<span class="string">"Training score"</span>)  </span><br><span class="line">    plt.plot(train_sizes, test_scores_mean,<span class="string">'o-'</span>,color=<span class="string">"g"</span>,  </span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)  </span><br><span class="line">    plt.legend(loc=<span class="string">"best"</span>)  </span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_learning_curve(LinearRegression(), <span class="string">'Liner_model'</span>, train_X[:<span class="number">1000</span>], train_y_ln[:<span class="number">1000</span>], ylim=(<span class="number">0.0</span>, <span class="number">0.5</span>), cv=<span class="number">5</span>, n_jobs=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>




<pre><code>&lt;module &apos;matplotlib.pyplot&apos; from &apos;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py&apos;&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321231918241.png" alt="54-1"></p>
<h4 id="5-4-3-多种模型对比"><a href="#5-4-3-多种模型对比" class="headerlink" title="5.4.3 多种模型对比"></a>5.4.3 多种模型对比</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train = sample_feature[continuous_feature_names + [<span class="string">'price'</span>]].dropna()</span><br><span class="line"></span><br><span class="line">train_X = train[continuous_feature_names]</span><br><span class="line">train_y = train[<span class="string">'price'</span>]</span><br><span class="line">train_y_ln = np.log(train_y + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h4 id="5-4-3-1-线性模型-amp-嵌入式特征选择"><a href="#5-4-3-1-线性模型-amp-嵌入式特征选择" class="headerlink" title="5.4.3 - 1 线性模型 &amp; 嵌入式特征选择"></a>5.4.3 - 1 线性模型 &amp; 嵌入式特征选择</h4><p>本章节默认，学习者已经了解关于过拟合、模型复杂度、正则化等概念。否则请寻找相关资料或参考如下连接：</p>
<ul>
<li>用简单易懂的语言描述「过拟合 overfitting」？ <a href="https://www.zhihu.com/question/32246256/answer/55320482" target="_blank" rel="noopener">https://www.zhihu.com/question/32246256/answer/55320482</a></li>
<li>模型复杂度与模型的泛化能力 <a href="http://yangyingming.com/article/434/" target="_blank" rel="noopener">http://yangyingming.com/article/434/</a></li>
<li>正则化的直观理解 <a href="https://blog.csdn.net/jinping_shi/article/details/52433975" target="_blank" rel="noopener">https://blog.csdn.net/jinping_shi/article/details/52433975</a></li>
</ul>
<p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。嵌入式选择最常用的是L1正则化与L2正则化。在对线性回归模型加入两种正则化方法后，他们分别变成了岭回归与Lasso回归。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">models = [LinearRegression(),</span><br><span class="line">          Ridge(),</span><br><span class="line">          Lasso()]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result = dict()</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = str(model).split(<span class="string">'('</span>)[<span class="number">0</span>]</span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] = scores</span><br><span class="line">    print(model_name + <span class="string">' is finished'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>LinearRegression is finished
Ridge is finished
Lasso is finished</code></pre><p>对三种方法的效果对比</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = pd.DataFrame(result)</span><br><span class="line">result.index = [<span class="string">'cv'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">result</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>Ridge</th>
      <th>Lasso</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cv1</th>
      <td>0.191642</td>
      <td>0.195665</td>
      <td>0.382708</td>
    </tr>
    <tr>
      <th>cv2</th>
      <td>0.194986</td>
      <td>0.198841</td>
      <td>0.383916</td>
    </tr>
    <tr>
      <th>cv3</th>
      <td>0.192737</td>
      <td>0.196629</td>
      <td>0.380754</td>
    </tr>
    <tr>
      <th>cv4</th>
      <td>0.195329</td>
      <td>0.199255</td>
      <td>0.385683</td>
    </tr>
    <tr>
      <th>cv5</th>
      <td>0.194450</td>
      <td>0.198173</td>
      <td>0.383555</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = LinearRegression().fit(train_X, train_y_ln)</span><br><span class="line">print(<span class="string">'intercept:'</span>+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure>

<pre><code>intercept:23.515984499017883





&lt;matplotlib.axes._subplots.AxesSubplot at 0x1feb933ca58&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321231945959.png" alt="output_65_2"></p>
<p>L2正则化在拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Ridge().fit(train_X, train_y_ln)</span><br><span class="line">print(<span class="string">'intercept:'</span>+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure>

<pre><code>intercept:5.901527844424091





&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fea9056860&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321232121957.png" alt="output_67_2"></p>
<p>L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。如下图，我们发现power与userd_time特征非常重要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Lasso().fit(train_X, train_y_ln)</span><br><span class="line">print(<span class="string">'intercept:'</span>+ str(model.intercept_))</span><br><span class="line">sns.barplot(abs(model.coef_), continuous_feature_names)</span><br></pre></td></tr></table></figure>

<pre><code>intercept:8.674427764003347





&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fea90b69b0&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/202003212321463.png" alt="output_69_2"></p>
<p>除此之外，决策树通过信息熵或GINI指数选择分裂节点时，优先选择的分裂特征也更加重要，这同样是一种特征选择的方法。XGBoost与LightGBM模型中的model_importance指标正是基于此计算的</p>
<h4 id="5-4-3-2-非线性模型"><a href="#5-4-3-2-非线性模型" class="headerlink" title="5.4.3 - 2 非线性模型"></a>5.4.3 - 2 非线性模型</h4><p>除了线性模型以外，还有许多我们常用的非线性模型如下，在此篇幅有限不再一一讲解原理。我们选择了部分常用模型与线性模型进行效果比对。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPRegressor</span><br><span class="line"><span class="keyword">from</span> xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> lightgbm.sklearn <span class="keyword">import</span> LGBMRegressor</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">models = [LinearRegression(),</span><br><span class="line">          DecisionTreeRegressor(),</span><br><span class="line">          RandomForestRegressor(),</span><br><span class="line">          GradientBoostingRegressor(),</span><br><span class="line">          MLPRegressor(solver=<span class="string">'lbfgs'</span>, max_iter=<span class="number">100</span>), </span><br><span class="line">          XGBRegressor(n_estimators = <span class="number">100</span>, objective=<span class="string">'reg:squarederror'</span>), </span><br><span class="line">          LGBMRegressor(n_estimators = <span class="number">100</span>)]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">result = dict()</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> models:</span><br><span class="line">    model_name = str(model).split(<span class="string">'('</span>)[<span class="number">0</span>]</span><br><span class="line">    scores = cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error))</span><br><span class="line">    result[model_name] = scores</span><br><span class="line">    print(model_name + <span class="string">' is finished'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>LinearRegression is finished
DecisionTreeRegressor is finished
RandomForestRegressor is finished
GradientBoostingRegressor is finished
MLPRegressor is finished
XGBRegressor is finished
LGBMRegressor is finished</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = pd.DataFrame(result)</span><br><span class="line">result.index = [<span class="string">'cv'</span> + str(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">result</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LinearRegression</th>
      <th>DecisionTreeRegressor</th>
      <th>RandomForestRegressor</th>
      <th>GradientBoostingRegressor</th>
      <th>MLPRegressor</th>
      <th>XGBRegressor</th>
      <th>LGBMRegressor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cv1</th>
      <td>0.191642</td>
      <td>0.184566</td>
      <td>0.136266</td>
      <td>0.168626</td>
      <td>124.299426</td>
      <td>0.168698</td>
      <td>0.141159</td>
    </tr>
    <tr>
      <th>cv2</th>
      <td>0.194986</td>
      <td>0.187029</td>
      <td>0.139693</td>
      <td>0.171905</td>
      <td>257.886236</td>
      <td>0.172258</td>
      <td>0.143363</td>
    </tr>
    <tr>
      <th>cv3</th>
      <td>0.192737</td>
      <td>0.184839</td>
      <td>0.136871</td>
      <td>0.169553</td>
      <td>236.829589</td>
      <td>0.168604</td>
      <td>0.142137</td>
    </tr>
    <tr>
      <th>cv4</th>
      <td>0.195329</td>
      <td>0.182605</td>
      <td>0.138689</td>
      <td>0.172299</td>
      <td>130.197264</td>
      <td>0.172474</td>
      <td>0.143461</td>
    </tr>
    <tr>
      <th>cv5</th>
      <td>0.194450</td>
      <td>0.186626</td>
      <td>0.137420</td>
      <td>0.171206</td>
      <td>268.090236</td>
      <td>0.170898</td>
      <td>0.141921</td>
    </tr>
  </tbody>
</table>
</div>



<p>可以看到随机森林模型在每一个fold中均取得了更好的效果</p>
<h4 id="5-4-4-模型调参"><a href="#5-4-4-模型调参" class="headerlink" title="5.4.4  模型调参"></a>5.4.4  模型调参</h4><p>在此我们介绍了三种常用的调参方法如下：</p>
<ul>
<li>贪心算法 <a href="https://www.jianshu.com/p/ab89df9759c8" target="_blank" rel="noopener">https://www.jianshu.com/p/ab89df9759c8</a></li>
<li>网格调参 <a href="https://blog.csdn.net/weixin_43172660/article/details/83032029" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43172660/article/details/83032029</a></li>
<li>贝叶斯调参 <a href="https://blog.csdn.net/linxid/article/details/81189154" target="_blank" rel="noopener">https://blog.csdn.net/linxid/article/details/81189154</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## LGB的参数集合：</span></span><br><span class="line"></span><br><span class="line">objective = [<span class="string">'regression'</span>, <span class="string">'regression_l1'</span>, <span class="string">'mape'</span>, <span class="string">'huber'</span>, <span class="string">'fair'</span>]</span><br><span class="line"></span><br><span class="line">num_leaves = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">max_depth = [<span class="number">3</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">15</span>,<span class="number">20</span>,<span class="number">40</span>, <span class="number">55</span>]</span><br><span class="line">bagging_fraction = []</span><br><span class="line">feature_fraction = []</span><br><span class="line">drop_rate = []</span><br></pre></td></tr></table></figure>

<h4 id="5-4-4-1-贪心调参"><a href="#5-4-4-1-贪心调参" class="headerlink" title="5.4.4 - 1 贪心调参"></a>5.4.4 - 1 贪心调参</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">best_obj = dict()</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line">    </span><br><span class="line">best_leaves = dict()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line">    </span><br><span class="line">best_depth = dict()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=min(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=min(best_leaves.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br><span class="line">    best_depth[depth] = score</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.lineplot(x=[<span class="string">'0_initial'</span>,<span class="string">'1_turning_obj'</span>,<span class="string">'2_turning_leaves'</span>,<span class="string">'3_turning_depth'</span>], y=[<span class="number">0.143</span> ,min(best_obj.values()), min(best_leaves.values()), min(best_depth.values())])</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fea93f6080&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321232159934.png" alt="83-1"></p>
<h4 id="5-4-4-2-Grid-Search-调参"><a href="#5-4-4-2-Grid-Search-调参" class="headerlink" title="5.4.4 - 2 Grid Search 调参"></a>5.4.4 - 2 Grid Search 调参</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters = &#123;<span class="string">'objective'</span>: objective , <span class="string">'num_leaves'</span>: num_leaves, <span class="string">'max_depth'</span>: max_depth&#125;</span><br><span class="line">model = LGBMRegressor()</span><br><span class="line">clf = GridSearchCV(model, parameters, cv=<span class="number">5</span>)</span><br><span class="line">clf = clf.fit(train_X, train_y)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.best_params_</span><br></pre></td></tr></table></figure>




<pre><code>{&apos;max_depth&apos;: 15, &apos;num_leaves&apos;: 55, &apos;objective&apos;: &apos;regression&apos;}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = LGBMRegressor(objective=<span class="string">'regression'</span>,</span><br><span class="line">                          num_leaves=<span class="number">55</span>,</span><br><span class="line">                          max_depth=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.mean(cross_val_score(model, X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)))</span><br></pre></td></tr></table></figure>




<pre><code>0.13626164479243302</code></pre><h4 id="5-4-4-3-贝叶斯调参"><a href="#5-4-4-3-贝叶斯调参" class="headerlink" title="5.4.4 - 3 贝叶斯调参"></a>5.4.4 - 3 贝叶斯调参</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv</span><span class="params">(num_leaves, max_depth, subsample, min_child_samples)</span>:</span></span><br><span class="line">    val = cross_val_score(</span><br><span class="line">        LGBMRegressor(objective = <span class="string">'regression_l1'</span>,</span><br><span class="line">            num_leaves=int(num_leaves),</span><br><span class="line">            max_depth=int(max_depth),</span><br><span class="line">            subsample = subsample,</span><br><span class="line">            min_child_samples = int(min_child_samples)</span><br><span class="line">        ),</span><br><span class="line">        X=train_X, y=train_y_ln, verbose=<span class="number">0</span>, cv = <span class="number">5</span>, scoring=make_scorer(mean_absolute_error)</span><br><span class="line">    ).mean()</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - val</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rf_bo = BayesianOptimization(</span><br><span class="line">    rf_cv,</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="string">'num_leaves'</span>: (<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">    <span class="string">'max_depth'</span>: (<span class="number">2</span>, <span class="number">100</span>),</span><br><span class="line">    <span class="string">'subsample'</span>: (<span class="number">0.1</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="string">'min_child_samples'</span> : (<span class="number">2</span>, <span class="number">100</span>)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rf_bo.maximize()</span><br></pre></td></tr></table></figure>

<pre><code>|   iter    |  target   | max_depth | min_ch... | num_le... | subsample |
-------------------------------------------------------------------------
| [0m 1       [0m | [0m 0.8649  [0m | [0m 89.57   [0m | [0m 47.3    [0m | [0m 55.13   [0m | [0m 0.1792  [0m |
| [0m 2       [0m | [0m 0.8477  [0m | [0m 99.86   [0m | [0m 60.91   [0m | [0m 15.35   [0m | [0m 0.4716  [0m |
| [95m 3       [0m | [95m 0.8698  [0m | [95m 81.74   [0m | [95m 83.32   [0m | [95m 92.59   [0m | [95m 0.9559  [0m |
| [0m 4       [0m | [0m 0.8627  [0m | [0m 90.2    [0m | [0m 8.754   [0m | [0m 43.34   [0m | [0m 0.7772  [0m |
| [0m 5       [0m | [0m 0.8115  [0m | [0m 10.07   [0m | [0m 86.15   [0m | [0m 4.109   [0m | [0m 0.3416  [0m |
| [95m 6       [0m | [95m 0.8701  [0m | [95m 99.15   [0m | [95m 9.158   [0m | [95m 99.47   [0m | [95m 0.494   [0m |
| [0m 7       [0m | [0m 0.806   [0m | [0m 2.166   [0m | [0m 2.416   [0m | [0m 97.7    [0m | [0m 0.224   [0m |
| [0m 8       [0m | [0m 0.8701  [0m | [0m 98.57   [0m | [0m 97.67   [0m | [0m 99.87   [0m | [0m 0.3703  [0m |
| [95m 9       [0m | [95m 0.8703  [0m | [95m 99.87   [0m | [95m 43.03   [0m | [95m 99.72   [0m | [95m 0.9749  [0m |
| [0m 10      [0m | [0m 0.869   [0m | [0m 10.31   [0m | [0m 99.63   [0m | [0m 99.34   [0m | [0m 0.2517  [0m |
| [95m 11      [0m | [95m 0.8703  [0m | [95m 52.27   [0m | [95m 99.56   [0m | [95m 98.97   [0m | [95m 0.9641  [0m |
| [0m 12      [0m | [0m 0.8669  [0m | [0m 99.89   [0m | [0m 8.846   [0m | [0m 66.49   [0m | [0m 0.1437  [0m |
| [0m 13      [0m | [0m 0.8702  [0m | [0m 68.13   [0m | [0m 75.28   [0m | [0m 98.71   [0m | [0m 0.153   [0m |
| [0m 14      [0m | [0m 0.8695  [0m | [0m 84.13   [0m | [0m 86.48   [0m | [0m 91.9    [0m | [0m 0.7949  [0m |
| [0m 15      [0m | [0m 0.8702  [0m | [0m 98.09   [0m | [0m 59.2    [0m | [0m 99.65   [0m | [0m 0.3275  [0m |
| [0m 16      [0m | [0m 0.87    [0m | [0m 68.97   [0m | [0m 98.62   [0m | [0m 98.93   [0m | [0m 0.2221  [0m |
| [0m 17      [0m | [0m 0.8702  [0m | [0m 99.85   [0m | [0m 63.74   [0m | [0m 99.63   [0m | [0m 0.4137  [0m |
| [0m 18      [0m | [0m 0.8703  [0m | [0m 45.87   [0m | [0m 99.05   [0m | [0m 99.89   [0m | [0m 0.3238  [0m |
| [0m 19      [0m | [0m 0.8702  [0m | [0m 79.65   [0m | [0m 46.91   [0m | [0m 98.61   [0m | [0m 0.8999  [0m |
| [0m 20      [0m | [0m 0.8702  [0m | [0m 99.25   [0m | [0m 36.73   [0m | [0m 99.05   [0m | [0m 0.1262  [0m |
| [0m 21      [0m | [0m 0.8702  [0m | [0m 85.51   [0m | [0m 85.34   [0m | [0m 99.77   [0m | [0m 0.8917  [0m |
| [0m 22      [0m | [0m 0.8696  [0m | [0m 99.99   [0m | [0m 38.51   [0m | [0m 89.13   [0m | [0m 0.9884  [0m |
| [0m 23      [0m | [0m 0.8701  [0m | [0m 63.29   [0m | [0m 97.93   [0m | [0m 99.94   [0m | [0m 0.9585  [0m |
| [0m 24      [0m | [0m 0.8702  [0m | [0m 93.04   [0m | [0m 71.42   [0m | [0m 99.94   [0m | [0m 0.9646  [0m |
| [0m 25      [0m | [0m 0.8701  [0m | [0m 99.73   [0m | [0m 16.21   [0m | [0m 99.38   [0m | [0m 0.9778  [0m |
| [0m 26      [0m | [0m 0.87    [0m | [0m 86.28   [0m | [0m 58.1    [0m | [0m 99.47   [0m | [0m 0.107   [0m |
| [0m 27      [0m | [0m 0.8703  [0m | [0m 47.28   [0m | [0m 99.83   [0m | [0m 99.65   [0m | [0m 0.4674  [0m |
| [0m 28      [0m | [0m 0.8703  [0m | [0m 68.29   [0m | [0m 99.51   [0m | [0m 99.4    [0m | [0m 0.2757  [0m |
| [0m 29      [0m | [0m 0.8701  [0m | [0m 76.49   [0m | [0m 73.41   [0m | [0m 99.86   [0m | [0m 0.9394  [0m |
| [0m 30      [0m | [0m 0.8695  [0m | [0m 37.27   [0m | [0m 99.87   [0m | [0m 89.87   [0m | [0m 0.7588  [0m |
=========================================================================</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> - rf_bo.max[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure>




<pre><code>0.1296693644053145</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本章中，我们完成了建模与调参的工作，并对我们的模型进行了验证。此外，我们还采用了一些基本方法来提高预测的精度，提升如下图所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">13</span>,<span class="number">5</span>))</span><br><span class="line">sns.lineplot(x=[<span class="string">'0_origin'</span>,<span class="string">'1_log_transfer'</span>,<span class="string">'2_L1_&amp;_L2'</span>,<span class="string">'3_change_model'</span>,<span class="string">'4_parameter_turning'</span>], y=[<span class="number">1.36</span> ,<span class="number">0.19</span>, <span class="number">0.19</span>, <span class="number">0.14</span>, <span class="number">0.13</span>])</span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1feac73ceb8&gt;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200321232216795.png" alt="98-1"></p>
<p><strong>Task5 建模调参 END.</strong></p>
<p>baseline来自：<a href="https://github.com/datawhalechina/team-learning/edit/master/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%EF%BC%89/Task4%20%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82%20.md" target="_blank" rel="noopener">https://github.com/datawhalechina/team-learning/edit/master/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AE%9E%E8%B7%B5%EF%BC%88%E4%BA%8C%E6%89%8B%E8%BD%A6%E4%BB%B7%E6%A0%BC%E9%A2%84%E6%B5%8B%EF%BC%89/Task4%20%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82%20.md</a></p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Chloe</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://childgrown.github.io/2020/04/01/Task4-Modeling/">https://childgrown.github.io/2020/04/01/Task4-Modeling/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E5%BB%BA%E6%A8%A1/"># 建模</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/04/21/creeper-task-1/">creeper-task-1</a>
            
            
            <a class="next" rel="next" href="/2020/03/28/Task3-feature-engineering/">Task3-feature-engineering</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Chloe | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
